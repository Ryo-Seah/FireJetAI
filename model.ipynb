{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06fbc209-df0f-46ea-b69c-a51642da1270",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tag</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>..\\..\\..\\data\\cropped-by-semantic-tag\\0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>..\\..\\..\\data\\cropped-by-semantic-tag\\1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>header</td>\n",
       "      <td>..\\..\\..\\data\\cropped-by-semantic-tag\\2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>button</td>\n",
       "      <td>..\\..\\..\\data\\cropped-by-semantic-tag\\3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>button</td>\n",
       "      <td>..\\..\\..\\data\\cropped-by-semantic-tag\\4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     tag                                     img_path\n",
       "0   0       a  ..\\..\\..\\data\\cropped-by-semantic-tag\\0.png\n",
       "1   1       a  ..\\..\\..\\data\\cropped-by-semantic-tag\\1.png\n",
       "2   2  header  ..\\..\\..\\data\\cropped-by-semantic-tag\\2.png\n",
       "3   3  button  ..\\..\\..\\data\\cropped-by-semantic-tag\\3.png\n",
       "4   4  button  ..\\..\\..\\data\\cropped-by-semantic-tag\\4.png"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/cropped-by-semantic-tag/_images_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to understand its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dae38459-a561-4b1a-805b-cfb00c6706d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9323, 1998, 1998)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and remaining data with a 70-30 split\n",
    "train_data, remaining_data = train_test_split(data, test_size=0.3, stratify=data['tag'], random_state=42)\n",
    "\n",
    "# Split the remaining data equally into validation and test sets\n",
    "validation_data, test_data = train_test_split(remaining_data, test_size=0.5, stratify=remaining_data['tag'], random_state=42)\n",
    "\n",
    "# Display the size of each set\n",
    "(len(train_data), len(validation_data), len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92124583-beff-45b8-a792-bd874e8046f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "IMG_WIDTH, IMG_HEIGHT = 150, 150  # Define the target width and height of images\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Rescale pixel values from [0, 255] to [0, 1]\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Note: No data augmentation should be applied to validation and test sets\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4566b80b-9e15-4325-a9f2-e8ca0882c2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9323 validated image filenames belonging to 11 classes.\n",
      "Found 1998 validated image filenames belonging to 11 classes.\n",
      "Found 1998 validated image filenames belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# //Create generators\n",
    "def update_image_path(old_path):\n",
    "    # Split the path and get the last part (the filename)\n",
    "    filename = old_path.split('\\\\')[-1]\n",
    "    # Construct the new path\n",
    "    new_path = f'data/cropped-by-semantic-tag/{filename}'\n",
    "    return new_path\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_data,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),  # Adjust based on your image size\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=validation_data,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_data,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "# Apply this transformation to each DataFrame\n",
    "train_data['img_path'] = train_data['img_path']\n",
    "# validation_data['img_path'] = validation_data['img_path'].apply(update_image_path)\n",
    "# test_data['img_path'] = test_data['img_path'].apply(update_image_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "692ece9b-abd8-418c-9114-1e519f8fb3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Tag Counts:\n",
      "tag\n",
      "h3          1885\n",
      "button      1837\n",
      "h2          1656\n",
      "a           1376\n",
      "input        863\n",
      "h4           573\n",
      "header       373\n",
      "h1           303\n",
      "form         288\n",
      "footer       161\n",
      "textarea       8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation Data Tag Counts:\n",
      "tag\n",
      "h3          404\n",
      "button      394\n",
      "h2          354\n",
      "a           295\n",
      "input       185\n",
      "h4          123\n",
      "header       80\n",
      "h1           65\n",
      "form         62\n",
      "footer       34\n",
      "textarea      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test Data Tag Counts:\n",
      "tag\n",
      "h3          404\n",
      "button      394\n",
      "h2          355\n",
      "a           295\n",
      "input       185\n",
      "h4          122\n",
      "header       80\n",
      "h1           65\n",
      "form         61\n",
      "footer       35\n",
      "textarea      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_tag_counts = train_data['tag'].value_counts()\n",
    "print(\"Training Data Tag Counts:\")\n",
    "print(train_tag_counts)\n",
    "\n",
    "# Count instances of each tag in the validation data\n",
    "validation_tag_counts = validation_data['tag'].value_counts()\n",
    "print(\"\\nValidation Data Tag Counts:\")\n",
    "print(validation_tag_counts)\n",
    "\n",
    "# Count instances of each tag in the test data\n",
    "test_tag_counts = test_data['tag'].value_counts()\n",
    "print(\"\\nTest Data Tag Counts:\")\n",
    "print(test_tag_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "016ddfe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631        data/cropped-by-semantic-tag/631.png\n",
      "8262      data/cropped-by-semantic-tag/8262.png\n",
      "9380      data/cropped-by-semantic-tag/9380.png\n",
      "1609      data/cropped-by-semantic-tag/1609.png\n",
      "2929      data/cropped-by-semantic-tag/2929.png\n",
      "                          ...                  \n",
      "5797      data/cropped-by-semantic-tag/5797.png\n",
      "12296    data/cropped-by-semantic-tag/12296.png\n",
      "8041      data/cropped-by-semantic-tag/8041.png\n",
      "10035    data/cropped-by-semantic-tag/10035.png\n",
      "8041      data/cropped-by-semantic-tag/8041.png\n",
      "Name: img_path, Length: 20735, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to oversample a dataset\n",
    "def oversample_dataset(dataset):\n",
    "    majority_class_size = dataset['tag'].value_counts().max()\n",
    "    oversampled_data = pd.DataFrame()\n",
    "\n",
    "    for tag, group in dataset.groupby('tag'):\n",
    "        oversampled_group = resample(group,\n",
    "                                      replace=True,\n",
    "                                      n_samples=majority_class_size,\n",
    "                                      random_state=42)\n",
    "        oversampled_data = pd.concat([oversampled_data, oversampled_group])\n",
    "\n",
    "    return oversampled_data\n",
    "\n",
    "oversampled_train = oversample_dataset(train_data)\n",
    "oversampled_validation = oversample_dataset(validation_data)\n",
    "oversampled_test = oversample_dataset(test_data)\n",
    "\n",
    "# Now, `oversampled_data` contains a balanced dataset\n",
    "\n",
    "print(oversampled_train['img_path'])\n",
    "# print(oversampled_validation['tag'].value_counts())\n",
    "# print(oversampled_test['tag'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d91984f0-6b22-40da-9637-c4a2818f7d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20735 validated image filenames belonging to 11 classes.\n",
      "Found 4444 validated image filenames belonging to 11 classes.\n",
      "Found 4444 validated image filenames belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "# //Create generators\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=oversampled_train,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),  # Adjust based on your image size\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    dataframe=oversampled_validation,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=oversampled_test,\n",
    "    x_col='img_path',\n",
    "    y_col='tag',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4d2f6293-0535-46c2-b503-6b9c69de5268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "671d1cef-27a5-4815-8929-40311cc55307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "648/648 [==============================] - 526s 811ms/step - loss: 2.1032 - accuracy: 0.2717 - val_loss: 2.0499 - val_accuracy: 0.3231\n",
      "Epoch 2/10\n",
      "648/648 [==============================] - 531s 819ms/step - loss: 3.8215 - accuracy: 0.2850 - val_loss: 11.2266 - val_accuracy: 0.2144\n",
      "Epoch 3/10\n",
      "648/648 [==============================] - 535s 826ms/step - loss: 14.2200 - accuracy: 0.2504 - val_loss: 76.8852 - val_accuracy: 0.2363\n",
      "Epoch 4/10\n",
      "648/648 [==============================] - 521s 804ms/step - loss: 208.1691 - accuracy: 0.2409 - val_loss: 705.7473 - val_accuracy: 0.2786\n",
      "Epoch 5/10\n",
      "648/648 [==============================] - 523s 807ms/step - loss: 1743.5170 - accuracy: 0.2385 - val_loss: 3905.9438 - val_accuracy: 0.2315\n",
      "Epoch 6/10\n",
      "648/648 [==============================] - 638s 986ms/step - loss: 10763.0498 - accuracy: 0.2432 - val_loss: 21128.1543 - val_accuracy: 0.3092\n",
      "Epoch 7/10\n",
      "648/648 [==============================] - 525s 810ms/step - loss: 46572.2188 - accuracy: 0.2452 - val_loss: 77851.1016 - val_accuracy: 0.2430\n",
      "Epoch 8/10\n",
      "648/648 [==============================] - 527s 814ms/step - loss: 110797.2266 - accuracy: 0.2490 - val_loss: 152865.4062 - val_accuracy: 0.2419\n",
      "Epoch 9/10\n",
      "648/648 [==============================] - 617s 952ms/step - loss: 235427.4531 - accuracy: 0.2452 - val_loss: 323165.2500 - val_accuracy: 0.2455\n",
      "Epoch 10/10\n",
      "648/648 [==============================] - 522s 805ms/step - loss: 361546.8125 - accuracy: 0.2487 - val_loss: 666502.3125 - val_accuracy: 0.2081\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,  \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91d778be-6dce-4f80-b3d6-a65b21ea4f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f41dd-8f3b-4488-ba8b-cd3c2eeae844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageclassi",
   "language": "python",
   "name": "imageclassi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
